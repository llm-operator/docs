Welcome to LLM Operator documentation!
======================================

.. raw:: html

   <p style="text-align:center">
   <a class="github-button" href="https://github.com/llm-operator/llm-operator" data-show-count="true" data-size="large" aria-label="Star llm-operator/llm-operator on GitHub">Star</a>
   <script async defer src="https://buttons.github.io/buttons.js"></script>
   </p>

   <p style="text-align:center">
   <strong>Transform your GPU clusters into a powerhouse for generative AI workloads</strong>
   </p>


LLM Operator builds a software stack that provides **LLM as a service**. It provides the
`OpenAI-compatible API <https://platform.openai.com/docs/api-reference>`_ anywhere, including the
following functionality:

- LLM fine-tuning job management
- LLM inference
- Fine-tuned models management
- Training/validation file management

LLM Operator also provides optimization technology on GPU, including the following functionality:

- Auto-scaling of inference-workloads
- Efficient scheduling of fine-tuning batch jobs
- GPU sharing

Check out the :doc:`getting_started` section for further information, including
how to install LLM Operator.

.. note::

   This project is under active development.

Contents
--------

.. toctree::

   getting_started
   tutorial
   inference
   rag
   fine_tuning
   jupyter_notebook
   training
   gpu_showback
   user_management
   access_control
   architecture
   multi_cluster_deployment
   integrations
   roadmap
